{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jmGTthpfAWEc",
        "kEUER_mLfsI5",
        "Dbvd_MNykxDV",
        "MB5lp5X0v5rQ"
      ],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic infromations**\n",
        "\n",
        "In each sentence, we have Token, Vocab, and sequence, that here we try to learn about them.\n",
        "To apply or extract this information, there are a few libraries which in the first step we will learn about \"Spacy\" library, which has ready-made models, languages, and methods for use. On the next step, we will try to understand how we are able to embed our text into a vector space."
      ],
      "metadata": {
        "id": "jmGTthpfAWEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Spacy**\n",
        "\n",
        "spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "It‚Äôs designed specifically for production use and helps you build applications that process and ‚Äúunderstand‚Äù large volumes of text.\n",
        "It can be used to build information extraction or natural language understanding systems.\n"
      ],
      "metadata": {
        "id": "7Bkv7qH_Dvyy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2Naob9VqDaSo"
      },
      "outputs": [],
      "source": [
        "# Install useful libraries for LLMs\n",
        "\n",
        "!pip install spacy\n",
        "\n",
        "'''Summon a specific dataset form spacy. Here we will download and use en_core_web_md (An English pipeline optimized for CPU).\n",
        "It also support the other languages (https://spacy.io/usage/models) like Persian and we will able to download them\n",
        "separately in multi language section (xx_sent_ud_sm is used for Persian).\n",
        "'''\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features of Spacy library:\n",
        "\n",
        "| **Name**                     | **Description** |\n",
        "|------------------------------|---------------|\n",
        "| **Tokenization**            | Segmenting text into words, punctuation marks, etc. |\n",
        "| **Part-of-speech (POS) Tagging** | Assigning word types to tokens (e.g., verb, noun). |\n",
        "| **Dependency Parsing**      | Assigning syntactic dependency labels (e.g., subject, object) to describe token relations. |\n",
        "| **Lemmatization**          | Assigning the base forms of words (e.g., \"was\" ‚Üí \"be\", \"rats\" ‚Üí \"rat\"). |\n",
        "| **Sentence Boundary Detection (SBD)** | Finding and segmenting individual sentences. |\n",
        "| **Named Entity Recognition (NER)** | Labelling named \"real-world\" objects (e.g., persons, companies, locations). |\n",
        "| **Entity Linking (EL)**    | Disambiguating textual entities to unique identifiers in a knowledge base. |\n",
        "| **Similarity**             | Comparing words, text spans, or documents to measure similarity. |\n",
        "| **Text Classification**    | Assigning categories/labels to a document or parts of a document. |\n",
        "| **Rule-based Matching**    | Finding token sequences based on text/linguistic patterns (like regex for NLP). |\n",
        "| **Training**              | Updating and improving a statistical model‚Äôs predictions. |\n",
        "| **Serialization**         | Saving objects (e.g., models, docs) to files or byte strings. |\n",
        "\n",
        "\n",
        "There are several libraries similar to **spaCy** for Natural Language Processing (NLP), each with its own strengths. Here are some popular alternatives:\n",
        "\n",
        "### **1. Hugging Face Transformers (ü§ó)**\n",
        "   - **Best for:** State-of-the-art (SOTA) transformer models (BERT, GPT, T5, etc.)\n",
        "   - **Features:**\n",
        "     - Pre-trained models for tasks like text classification, NER, summarization, translation.\n",
        "     - Easy fine-tuning with `pipeline()` API.\n",
        "     - Supports PyTorch & TensorFlow.\n",
        "   - **Website:** [huggingface.co](https://huggingface.co)\n",
        "\n",
        "### **2. NLTK (Natural Language Toolkit)**\n",
        "   - **Best for:** Education, research, and basic NLP tasks.\n",
        "   - **Features:**\n",
        "     - Tokenization, stemming, POS tagging, parsing.\n",
        "     - Large collection of corpora and lexical resources.\n",
        "     - Less optimized for production than spaCy.\n",
        "   - **Website:** [nltk.org](https://www.nltk.org/)\n",
        "\n",
        "### **3. Stanza (by Stanford NLP)**\n",
        "   - **Best for:** Multilingual NLP with high accuracy.\n",
        "   - **Features:**\n",
        "     - Supports 70+ languages.\n",
        "     - Dependency parsing, NER, POS tagging.\n",
        "     - Built on PyTorch.\n",
        "   - **Website:** [stanfordnlp.github.io/stanza](https://stanfordnlp.github.io/stanza/)\n",
        "\n",
        "### **4. Flair (by Zalando Research)**\n",
        "   - **Best for:** Contextual embeddings & advanced NLP.\n",
        "   - **Features:**\n",
        "     - Built on PyTorch.\n",
        "     - Supports embeddings (BERT, ELMo, Flair).\n",
        "     - Good for NER and text classification.\n",
        "   - **Website:** [github.com/flairNLP/flair](https://github.com/flairNLP/flair)\n",
        "\n",
        "### **5. Gensim**\n",
        "   - **Best for:** Topic modeling & word embeddings.\n",
        "   - **Features:**\n",
        "     - Implements Word2Vec, Doc2Vec, FastText.\n",
        "     - LDA for topic modeling.\n",
        "     - Not for deep learning tasks.\n",
        "   - **Website:** [radimrehurek.com/gensim](https://radimrehurek.com/gensim/)\n",
        "\n",
        "### **6. AllenNLP**\n",
        "   - **Best for:** Research & custom deep learning NLP models.\n",
        "   - **Features:**\n",
        "     - Built on PyTorch.\n",
        "     - High-level API for NLP tasks.\n",
        "     - Good for prototyping new models.\n",
        "   - **Website:** [allennlp.org](https://allennlp.org/)\n",
        "\n",
        "### **7. TextBlob**\n",
        "   - **Best for:** Simple NLP tasks (beginners).\n",
        "   - **Features:**\n",
        "     - Built on NLTK & Pattern.\n",
        "     - Sentiment analysis, translation, noun phrase extraction.\n",
        "     - Easy-to-use API.\n",
        "   - **Website:** [textblob.readthedocs.io](https://textblob.readthedocs.io/)\n",
        "\n",
        "### **Comparison Table**\n",
        "| Library          | Best For                     | Deep Learning Support | Production Ready | Multilingual |\n",
        "|------------------|-----------------------------|----------------------|------------------|--------------|\n",
        "| **spaCy**       | Fast, production NLP        | ‚úÖ (via extensions)  | ‚úÖ               | ‚úÖ (20+ langs) |\n",
        "| **Hugging Face**| SOTA transformers           | ‚úÖ (PyTorch/TF)      | ‚úÖ               | ‚úÖ (100+ langs) |\n",
        "| **NLTK**        | Education/research          | ‚ùå                   | ‚ùå               | ‚úÖ (limited) |\n",
        "| **Stanza**      | Accurate multilingual NLP   | ‚úÖ (PyTorch)         | ‚úÖ               | ‚úÖ (70+ langs) |\n",
        "| **Flair**       | Contextual embeddings       | ‚úÖ (PyTorch)         | ‚úÖ               | ‚úÖ (limited) |\n",
        "| **Gensim**      | Topic modeling/embeddings   | ‚ùå                   | ‚úÖ               | ‚úÖ (limited) |\n",
        "| **AllenNLP**    | Custom deep NLP models      | ‚úÖ (PyTorch)         | ‚ö†Ô∏è (research)   | ‚úÖ |\n",
        "| **TextBlob**    | Simple NLP tasks            | ‚ùå                   | ‚ùå               | ‚úÖ (limited) |\n",
        "\n",
        "### **Which One Should You Choose?**\n",
        "- **For production pipelines** ‚Üí **spaCy** (fast) or **Hugging Face** (transformers).\n",
        "- **For research/education** ‚Üí **NLTK**, **AllenNLP**, or **Flair**.\n",
        "- **For multilingual tasks** ‚Üí **Stanza** or **Hugging Face**.\n",
        "- **For embeddings & topic modeling** ‚Üí **Gensim**.\n"
      ],
      "metadata": {
        "id": "hdsvXev2lpyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and load dataset from spacy\n",
        "import numpy as np\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "''' The vectorisation process that Spacy has been on each word or symbol is semantic.\n",
        "It means the bus semantically will be  near to the car vector because they are sort of vehicle'''\n",
        "\n",
        "# Test a vocab vector\n",
        "print(nlp.vocab['bus'].vector,'\\n',100*'-')\n",
        "# Test Tokenizer\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "print(\"Tokeniz of input sentence: \",'\\n')\n",
        "for token in doc:\n",
        "    print(token.text)\n",
        "\n",
        "print(100*'-','\\n',\"Tokeniz of input sentence with other informations: \",'\\n')\n",
        "for token in doc:\n",
        "    print(token.text, token.has_vector, token.vector_norm)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lpcSZhtLFRtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocablury: Each word, punctuation, or the same token has a number called vocab\n",
        "doc = nlp(\"I love coffee\")\n",
        "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
        "print(doc.vocab.strings[3197928453018144401])  # 'coffee'"
      ],
      "metadata": {
        "id": "y070csE9yRsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Similarity between two vectors**\n",
        "\n",
        "Calculating Similarity between 2 vectors is one way to find their distance. This technique helps us determine whether semantically they are near each other. Because this is how vectorisation is done. There are many techniques [link](https://www.elastic.co/search-labs/blog/vector-similarity-techniques-and-scoring) for calculating similarity, including L1 distance, L2 distance, Cosine Similarity, Dot product similarity, and Max inner similarity, which will learn here about Cosine similarity in the following.\n"
      ],
      "metadata": {
        "id": "LyeK0SpQ1-Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Cosine similarity measures the similarity between two non-zero vectors by calculating the cosine of the angle between them. It is widely used in machine learning and data analysis, especially in text analysis, document comparison, search queries, and recommendation systems.\n",
        "\n",
        "Similarity measure calculates the distance between data objects based on their feature dimensions in a dataset.\n",
        "A smaller distance indicates a higher similarity, while a larger distance indicates a lower similarity.\n",
        "\n",
        "The formula to find the cosine similarity between two vectors is -\n",
        "\n",
        "Cs(x, y) = x . y / ||x|| √ó ||y||\n",
        "where,\n",
        "\n",
        "x . y = product (dot) of the vectors 'x' and 'y'.\n",
        "||x|| and ||y|| = length (magnitude) of the two vectors 'x' and 'y'.\n",
        "||x|| √ó ||y|| = regular product of the two vectors 'x' and 'y'.\n",
        "\n",
        "Example\n",
        "Consider an example to find the similarity between two vectors - 'x' and 'y', using Cosine Similarity.\n",
        "The 'x' vector has values, x = { 3, 2, 0, 5 } The 'y' vector has values, y = { 1, 0, 0, 0 } The formula for calculating the cosine similarity is :\n",
        "Cs(x, y) = x . y / ||x|| √ó ||y||\n",
        "\n",
        "x . y = 3*1 + 2*0 + 0*0 + 5*0 = 3\n",
        "\n",
        "||x|| = ‚àö (3)^2 + (2)^2 + (0)^2 + (5)^2 = 6.16\n",
        "\n",
        "||y|| = ‚àö (1)^2 + (0)^2 + (0)^2 + (0)^2 = 1\n",
        "\n",
        "Cs(x, y) = 3 / (6.16 * 1) = 0.49\n",
        "\n",
        "The dissimilarity between the two vectors 'x' and 'y' is given by 1 - (x, y) = 1 - 0.49 = 0.51\n",
        "'''\n",
        "\n",
        "def cosine_sim(v1,v2):\n",
        "  return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "\n",
        "bus_v = nlp.vocab['bus'].vector\n",
        "car_v = nlp.vocab['car'].vector\n",
        "cat_v = nlp.vocab['cat'].vector\n",
        "horse_v = nlp.vocab['horse'].vector\n",
        "\n",
        "print(f'This valuse shows the similarty value between car and bus: {cosine_sim(bus_v,car_v)}')\n",
        "print(f'This valuse shows the similarty value between car and cat: {cosine_sim(horse_v,car_v)}')\n",
        "\n",
        "print(\"Test the perform of similarity function with spacy --> Similarity between Bus and Car: \", nlp.vocab['bus'].similarity(nlp.vocab['car']))\n"
      ],
      "metadata": {
        "id": "it5igM6SJmSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
        "doc2 = nlp(\"Fast food tastes very good.\")\n",
        "\n",
        "# Similarity of two documents\n",
        "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
        "# Similarity of tokens and spans\n",
        "french_fries = doc1[2:4]\n",
        "burgers = doc1[5]\n",
        "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
      ],
      "metadata": {
        "id": "OtnQ5JWNsnMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embedding or Vectorization**\n",
        "\n",
        "Embedding or Vectorisation is a technique that makes our input data understandable to a computer by mapping input text into a vector space. there are a few libraries to do this like spacy-transformer but for having diversity and its streanghs we will try to learn about sentence transformer.\n",
        "\n",
        "Sentence Transformers is the go-to Python module for accessing, using, and training state-of-the-art embedding and reranker models. It can be used to compute embeddings using Sentence Transformer models or to calculate similarity scores using Cross-Encoder models. This unlocks a wide range of applications, including semantic search, semantic textual similarity, and paraphrase mining.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kEUER_mLfsI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The name and advantages of other libraries are in following:\n",
        "\n",
        "### **1. Hugging Face `sentence-transformers` (Official)**\n",
        "   - **Best for:** State-of-the-art (SOTA) sentence embeddings.\n",
        "   - **Features:**\n",
        "     - Built on top of Hugging Face Transformers.\n",
        "     - Pre-trained models (e.g., `all-MiniLM-L6-v2`, `mpnet-base`).\n",
        "     - Supports semantic search, clustering, and similarity tasks.\n",
        "   - **GitHub:** [github.com/UKPLab/sentence-transformers](https://github.com/UKPLab/sentence-transformers)\n",
        "\n",
        "### **2. Hugging Face Transformers (`pipeline` + Custom Models)**\n",
        "   - **Best for:** Using standalone models like `BERT`, `RoBERTa`, `T5` for embeddings.\n",
        "   - **Features:**\n",
        "     - Can extract embeddings from any Hugging Face model.\n",
        "     - Less optimized for sentence-level tasks than `sentence-transformers`.\n",
        "   - **Example:**\n",
        "\n",
        "\n",
        "### **3. FastText (by Facebook)**\n",
        "   - **Best for:** Word and sentence embeddings (especially for rare words).\n",
        "   - **Features:**\n",
        "     - Trains subword embeddings (good for morphologically rich languages).\n",
        "     - Can generate sentence embeddings by averaging word vectors.\n",
        "   - **GitHub:** [github.com/facebookresearch/fastText](https://github.com/facebookresearch/fastText)\n",
        "\n",
        "### **4. Gensim (`Doc2Vec`, `Word2Vec`)**\n",
        "   - **Best for:** Lightweight document/paragraph embeddings.\n",
        "   - **Features:**\n",
        "     - `Doc2Vec` for fixed-length document embeddings.\n",
        "     - No transformer-based SOTA, but fast for small datasets.\n",
        "\n",
        "### **5. Flair (Contextual Embeddings)**\n",
        "   - **Best for:** Advanced contextualized embeddings (e.g., `FlairEmbeddings`, `TransformerEmbeddings`).\n",
        "   - **Features:**\n",
        "     - Combines multiple embeddings (e.g., BERT + Flair).\n",
        "     - Good for downstream NLP tasks (NER, classification).\n",
        "   - **GitHub:** [github.com/flairNLP/flair](https://github.com/flairNLP/flair)\n",
        "\n",
        "### **6. TensorFlow Hub (Pre-trained Encoders)**\n",
        "   - **Best for:** Ready-to-use TF models for embeddings.\n",
        "   - **Features:**\n",
        "     - Hosts models like `Universal Sentence Encoder` (USE).\n",
        "     - One-line embedding extraction.\n",
        "\n",
        "\n",
        "### **7. spaCy (with `spacy-transformers`)**\n",
        "   - **Best for:** Embeddings within a production NLP pipeline.\n",
        "   - **Features:**\n",
        "     - Integrates Hugging Face models into spaCy.\n",
        "     - Supports sentence embeddings via `doc.vector` or `span.vector`.\n",
        "\n",
        "\n",
        "### **8. Jina AI (`Finetuner`)**\n",
        "   - **Best for:** Fine-tuning sentence embeddings for specific domains.\n",
        "   - **Features:**\n",
        "     - Optimizes `sentence-transformers` models for custom data.\n",
        "     - Focuses on search and retrieval tasks.\n",
        "   - **GitHub:** [github.com/jina-ai/finetuner](https://github.com/jina-ai/finetuner)\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison Table**\n",
        "\n",
        "| **Library**               | **Strengths** | **Sentence Transformers' Strengths** | **Transformer-Based?** | **Best Use Case** |\n",
        "|---------------------------|--------------|-------------------------------------|------------------------|------------------|\n",
        "| **Sentence Transformers** | SOTA sentence embeddings | ‚úÖ **Optimized for sentence-level tasks** (unlike raw Hugging Face models)<br>‚úÖ **Pre-trained models fine-tuned for similarity** (e.g., `all-mpnet-base-v2`)<br>‚úÖ **Built-in pooling** (no manual mean/max pooling needed)<br>‚úÖ **Semantic search/clustering support** (e.g., `util.cos_sim()`) | ‚úÖ | Semantic search, clustering, retrieval |\n",
        "| **Hugging Face (Raw Models)** | Flexible model usage | ‚ùå Requires manual pooling (e.g., `mean` of BERT outputs)<br>‚ùå Not fine-tuned for sentence similarity by default | ‚úÖ | Custom embedding extraction |\n",
        "| **FastText** | Subword embeddings, rare words | ‚ùå Word-level only (no native sentence embeddings)<br>‚ùå No transformer-based context | ‚ùå | Multilingual/word-level tasks |\n",
        "| **Gensim** | Lightweight Doc2Vec/Word2Vec | ‚ùå Bag-of-words style (no contextual embeddings)<br>‚ùå Outperformed by transformers | ‚ùå | Small-scale doc similarity |\n",
        "| **Flair** | Hybrid contextual embeddings | ‚ùå Focused on NER/classification, not sentence similarity | ‚úÖ (optional) | NER, classification |\n",
        "| **TF Hub (USE)** | Pre-trained Universal Sentence Encoder | ‚ùå Fixed models (less flexible than Sentence Transformers)<br>‚úÖ Good for quick prototypes | ‚úÖ | Quick sentence embeddings |\n",
        "| **spaCy + transformers** | Production NLP pipelines | ‚ùå Embeddings are side effect (not optimized for similarity)<br>‚úÖ Integrates with NLP tasks | ‚úÖ (with plugin) | Combined NLP + embeddings |\n",
        "| **Jina AI Finetuner** | Domain-specific fine-tuning | ‚úÖ **Extends Sentence Transformers** for custom data<br>‚úÖ Optimized for search/retrieval | ‚úÖ | Custom search systems |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tATo4PblLnag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insatall and import libraries\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "2Fj2cRXFeEQt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Sentence Transformer has many mapping models for vectorizing (https://sbert.net/docs/sentence_transformer/pretrained_models.html).\n",
        "Here we used one of the smallest model from pretrained section called all-MiniLM-L6-v2.\n",
        "This model has trained on 1 Billion data and it maps our input data to a 384 dimention vector'''\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Testing perfomance of embedding in all-MiniLM-L6-v2\n",
        "Text = [\n",
        "    'ŸÖŸÜ ÿ®Ÿá ÿ±ÿ≥ÿ™Ÿàÿ±ÿßŸÜ ŸÖÿ±ÿßÿ¨ÿπŸá ⁄©ÿ±ÿØŸÖ Ÿà ⁄©ÿ®ÿßÿ® ⁄©Ÿàÿ®€åÿØŸá ÿÆŸàÿ±ÿØŸÖ',\n",
        "    'ÿØÿ± ŸáŸÜ⁄ØÿßŸÖ ÿØÿ±ÿ≥ ÿÆŸàÿßŸÜÿØŸÜÿå ÿ¥ŸÜ€åÿØŸÜ ŸÖŸàÿ≥€åŸÇ€å ÿ¢ÿ±ÿßŸÖÿ¥ÿ®ÿÆÿ¥ ŸÖ€å ÿ™ŸàÿßŸÜÿØ ⁄©ŸÖ⁄© ⁄©ŸÜŸÜÿØŸá ÿ®ÿßÿ¥ÿØ',\n",
        "    'ÿßÿ≥ÿ™ÿßŸÜÿØÿßÿ±ÿØ Ÿáÿß€å ⁄©ÿßŸÅŸá ÿØÿ± ÿ≠Ÿàÿ≤Ÿá ŸÇŸáŸàŸá Ÿà ÿÆŸàÿ±ÿß⁄©€å ÿ®ÿßŸÑÿß ÿ±ŸÅÿ™Ÿá ÿßÿ≥ÿ™',\n",
        "    'ÿ®ÿ±ÿß€å ŸÇÿ®ŸàŸÑ€å ÿØÿ± ⁄©ŸÜ⁄©Ÿàÿ± ÿ®ÿß€åÿØ ÿ™ŸÑÿßÿ¥ ⁄©ŸÜ€åŸÖ Ÿà ⁄©ÿ™ÿßÿ® Ÿáÿß€å ŸÖÿÆÿ™ŸÑŸÅ ÿ±ÿß ⁄ÜŸÜÿØÿ®ÿßÿ± ŸÖÿ∑ÿßŸÑÿπŸá ⁄©ŸÜ€åŸÖ'\n",
        "]\n",
        "\n",
        "# Injecting text into the model for mapping in vector space\n",
        "Text_vec = model.encode(Text)"
      ],
      "metadata": {
        "id": "tUctcx1uhnEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chencking Similarity between each sentences by Cosine similarity\n",
        "print(f'This valuse shows the similarty value between sentence related to eating and sentence related to studing (1 & 2): {cosine_sim(Text_vec[0],Text_vec[1])}')\n",
        "print(f'This valuse shows the similarty value between sentence related to eating and sentence related to studing (3 & 4): {cosine_sim(Text_vec[2],Text_vec[3])}')\n",
        "print(f'This valuse shows the similarty value between sentence related to eating and sentence related to eating (1 & 3): {cosine_sim(Text_vec[0],Text_vec[2])}')\n",
        "print(f'This valuse shows the similarty value between sentence related to studing and sentence related to studing (2 & 4): {cosine_sim(Text_vec[1],Text_vec[3])}')\n",
        "\n",
        "print(\"As we can see model could detect relation and difference between the senctences but it doesn's have a good perfomance.\",'\\n',100*'-')\n",
        "\n",
        "# Checking embedding similarity by model.similarity in sentence transformer\n",
        "similarities = model.similarity(Text_vec, Text_vec)\n",
        "print(similarities)\n"
      ],
      "metadata": {
        "id": "J1-zoonugeB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing perfomance of embedding in distiluse-base-multilingual-cased-v2\n",
        "model_2 = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
        "\n",
        "# Injecting text into the model for mapping in vector space\n",
        "Text_vec = model_2.encode(Text)\n",
        "\n",
        "#Chencking Similarity between each sentences\n",
        "print(f'This valuse shows the similarty value between sentence related to eating and sentence related to studing (1 & 2): {cosine_sim(Text_vec[0],Text_vec[1])}')\n",
        "print(f'This valuse shows the similarty value between sentence related to eating and sentence related to studing (3 & 4): {cosine_sim(Text_vec[2],Text_vec[3])}')\n",
        "print(f'This valuse shows the similarty value between sentence related to eating and sentence related to eating (1 & 3): {cosine_sim(Text_vec[0],Text_vec[2])}')\n",
        "print(f'This valuse shows the similarty value between sentence related to studing and sentence related to studing (2 & 4): {cosine_sim(Text_vec[1],Text_vec[3])}')\n",
        "\n",
        "print(\"As we can see model could detect relation and difference between the senctences but it has a good perfomance.\",'\\n',100*'-')\n",
        "\n",
        "# Checking embedding similarity by model.similarity in sentence transformer\n",
        "similarities = model.similarity(Text_vec, Text_vec)\n",
        "print(similarities)"
      ],
      "metadata": {
        "id": "xo9MXYZHhfJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Making Simple sentence collection through vectorized data in previous section**\n",
        "\n",
        "The first step after doing embedding is making collection of words, sentences, and punctuations using database maker libraries like Chromadb, Milvest, and Faiss. Here we will do it by chromadb.\n",
        "**Chroma db** [link text](https://github.com/chroma-core/chroma). you can see some example here.\n",
        "The advantages of Chromadb as a database maker is the ability of combing with sentence-transformer library and sementic searching new input on entire collection.   "
      ],
      "metadata": {
        "id": "Dbvd_MNykxDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "import chromadb\n",
        "# Chromadb has some utils like embedding function that allows us to combine sentence transformer with this library\n",
        "from chromadb.utils import embedding_functions"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qMbfMEMYj7G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a path to save its output\n",
        "'''\n",
        "we can configure Chroma to save and load the database from your local machine, using the PersistentClient\n",
        "'''\n",
        "Client = chromadb.PersistentClient(path='/content/drive/MyDrive/Large Language Model (LLM)/Developing My Knowledge In LLM/FaraDars/')"
      ],
      "metadata": {
        "id": "TeP82uDll1EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As said before we can combine chroma with sentence-transformer.\n",
        "# This atribute allows us to use sentence-transformer models (https://www.sbert.net/docs/sentence_transformer/pretrained_models.html) in embedding process.\n",
        "# summon one of sentence-transform models as embedding function\n",
        "ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name='distiluse-base-multilingual-cased-v2')"
      ],
      "metadata": {
        "id": "MRYewjxZne_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make an empty database (collectio) that it includs distiluse-base-multilingual-cased-v2 embedding function model\n",
        "# metadata part in this function allows us to use some other techniques like searching methods like cosine distance at hnsw searching engine.\n",
        "# There are some other techniques insted of HNSW like PQ (Product quantization) or IVF (Inverted file index) or IVFPQ (Combination of IVF and PQ) but HNSW is faster than them.\n",
        "Collection = Client.create_collection(name=\"Sina\", embedding_function= ef, metadata={\"hnsw:space\": \"cosine\"})\n",
        "\n",
        "# Our Data\n",
        "Text = [\n",
        "    'ŸÖŸÜ ÿ®Ÿá ÿ±ÿ≥ÿ™Ÿàÿ±ÿßŸÜ ŸÖÿ±ÿßÿ¨ÿπŸá ⁄©ÿ±ÿØŸÖ Ÿà ⁄©ÿ®ÿßÿ® ⁄©Ÿàÿ®€åÿØŸá ÿÆŸàÿ±ÿØŸÖ',\n",
        "    'ÿØÿ± ŸáŸÜ⁄ØÿßŸÖ ÿØÿ±ÿ≥ ÿÆŸàÿßŸÜÿØŸÜÿå ÿ¥ŸÜ€åÿØŸÜ ŸÖŸàÿ≥€åŸÇ€å ÿ¢ÿ±ÿßŸÖÿ¥ÿ®ÿÆÿ¥ ŸÖ€å ÿ™ŸàÿßŸÜÿØ ⁄©ŸÖ⁄© ⁄©ŸÜŸÜÿØŸá ÿ®ÿßÿ¥ÿØ',\n",
        "    'ÿßÿ≥ÿ™ÿßŸÜÿØÿßÿ±ÿØ Ÿáÿß€å ⁄©ÿßŸÅŸá ÿØÿ± ÿ≠Ÿàÿ≤Ÿá ŸÇŸáŸàŸá Ÿà ÿÆŸàÿ±ÿß⁄©€å ÿ®ÿßŸÑÿß ÿ±ŸÅÿ™Ÿá ÿßÿ≥ÿ™',\n",
        "    'ÿ®ÿ±ÿß€å ŸÇÿ®ŸàŸÑ€å ÿØÿ± ⁄©ŸÜ⁄©Ÿàÿ± ÿ®ÿß€åÿØ ÿ™ŸÑÿßÿ¥ ⁄©ŸÜ€åŸÖ Ÿà ⁄©ÿ™ÿßÿ® Ÿáÿß€å ŸÖÿÆÿ™ŸÑŸÅ ÿ±ÿß ⁄ÜŸÜÿØÿ®ÿßÿ± ŸÖÿ∑ÿßŸÑÿπŸá ⁄©ŸÜ€åŸÖ'\n",
        "]\n",
        "\n",
        "# By ids we will able to assign a specific id for each sentences\n",
        "Collection.add(documents=Text, ids=[f'id_{i}' for i in range(len(Text))])"
      ],
      "metadata": {
        "id": "hJyswyF_oGu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with new text. Indeed, in this searching, a semantic searching will performed on the entire data and return ID and document that sementically is near to our input.\n",
        "# Query is an attribute that makes a searching process on the collection.\n",
        "# n_results leads to the return number of documents that are semantically near the input, which we set to 1.\n",
        "\n",
        "query_results = Collection.query(query_texts=['ŸÇÿ±ŸÖŸá ÿ≥ÿ®ÿ≤€å'], n_results=1)\n",
        "query_results"
      ],
      "metadata": {
        "id": "rWPlRgtfrDXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Making Database through text (PDFs) that we have.**\n",
        "\n",
        "Here we will try do collecting process by PDFs that we have on local system.\n",
        "In this process we will READ, SPLIT, and SAVE our documents in our path."
      ],
      "metadata": {
        "id": "MX5QQVoZ8uJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insatall useful libraries\n",
        "!pip install langchain langchain_community pypdf chromadb langchain_huggingface\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "-NVmquKg9CKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To read our PDF documents we need to import few libraries like PyPDFDirectoryLoader\n",
        "# To Split our texts into chunks or paragraphs we need to import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "GPk2CVy_9DAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determining our data path\n",
        "DATA_PATH = r\"/content/drive/MyDrive/Large Language Model (LLM)/Developing My Knowledge In LLM/FaraDars/Data\"\n",
        "\n",
        "def load_documents():\n",
        "    '''\n",
        "    Load document is a function that allows us to read all pdfs in our directory.\n",
        "    It takes data path and return documents.\n",
        "    The type of output should be a list with the information of all pdfs\n",
        "    '''\n",
        "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
        "    return document_loader.load()\n",
        "\n",
        "documents = load_documents()\n",
        "print(documents)\n",
        "print(\"\\n\",\"The type of out is: \",type(documents))"
      ],
      "metadata": {
        "id": "X_bfRhcI9C1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We need to inject our data into the model so that we can answer questions by them.\n",
        "To do this, one of methods is splitting all text into sub-text (separate paragraphs or chunks).\n",
        "So, we have a document and we must turn it into little chunks. We will able to do it by \"RecursiveCharacterTextSplitter\".\n",
        "In this method we can determine how much will be each chunk or paragraph.\n",
        "The reason of using this technique is that we should make a database (a vector based database).\n",
        "'''\n",
        "def split_text(documents: list[Document]):\n",
        "  '''\n",
        "  split_text function takes documents that we have and it return chunks (paragraphs).\n",
        "  In this function we use RecursiveCharacterTextSplitter as text splitter.\n",
        "  '''\n",
        "\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=400, # split text with 400 word into chunks\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "\n",
        "    document = chunks[10]\n",
        "    print(document.page_content)\n",
        "    print(document.metadata)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "chunks = split_text(documents)\n",
        "print('\\n',100*'-')\n",
        "for chunk in chunks[:10]:\n",
        "    print(chunk)"
      ],
      "metadata": {
        "id": "RI-zo1qa9Cyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making collection\n",
        "# apply vectorization method (Chromadb) on our dataset to make a database.\n",
        "# insted of using sentence-transformer, we will use HuggingFaceEmbeddings for vectorization.\n",
        "\n",
        "CHROMA_PATH = \"/content/drive/MyDrive/Large Language Model (LLM)/Developing My Knowledge In LLM/FaraDars/Data/chromadb\"\n",
        "def save_to_chroma(chunks: list[Document]):\n",
        "    '''\n",
        "    Here we will try to make a collection by chroma. In this function initially we check our path to save.\n",
        "    Then by Chroma.from_documents that takes prepared chunks and embedding technique and path it will make our database.\n",
        "    '''\n",
        "    if os.path.exists(CHROMA_PATH):\n",
        "        shutil.rmtree(CHROMA_PATH) # The shutil module offers a number of high-level operations on files and collections of files.\n",
        "\n",
        "    db = Chroma.from_documents(\n",
        "        chunks, HuggingFaceEmbeddings(), persist_directory=CHROMA_PATH\n",
        "    ) # here we used HuggingFaceEmbeddings insted of sentence transformer method.\n",
        "    db.persist() # The persist() method in ChromaDB is used to save the vector database (index) to disk so that it can be reloaded later without reprocessing the documents.\n",
        "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
      ],
      "metadata": {
        "id": "NJQ6o05X9xl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data_store():\n",
        "    '''\n",
        "    generate_data_store is a function that integrat our 3 function (read, split, and save as vector)\n",
        "    '''\n",
        "    documents = load_documents()\n",
        "    chunks = split_text(documents)\n",
        "    save_to_chroma(chunks)\n",
        "\n",
        "generate_data_store()"
      ],
      "metadata": {
        "id": "BE25N3qn9xij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It is just a test to see how HuggingFaceEmbeddings work\n",
        "ex = \"apple\"\n",
        "ex_1 = \"orange\"\n",
        "ex_2 = \"iphone\"\n",
        "\n",
        "embedding_function = HuggingFaceEmbeddings()\n",
        "vector = embedding_function.embed_query(ex)\n",
        "vector_1 = embedding_function.embed_query(ex_1)\n",
        "vector_2 = embedding_function.embed_query(ex_2)\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "C2z7KvVC9xgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0hH6zRQZqHgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Developing Models**\n",
        "Here we will try to develope some language models by HuggingFace and LangChain"
      ],
      "metadata": {
        "id": "DwR0NgEKmeax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A definition of Multi and Single Stage Task**\n",
        "\n",
        "In LLMs we have sort of two type of task called Multi Stage and Single stage task. The difference between them is that in Single stage we will achieve to our output directly but in multi stage we will try to break-down our task into sub-tasks. So, the aim of this technique is that break down our task into sub-taks and use them to extrac final result like sentiment of an article.\n",
        "Insted of using one single shot, first we can extract summary of article and then by injecting this summary to other agent we will able to calculate emotion of it.\n",
        "To do this, we used some models that will perform our tasks."
      ],
      "metadata": {
        "id": "MB5lp5X0v5rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "# This library includes multitute of Large Language models"
      ],
      "metadata": {
        "id": "G2bhmbwXsBjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "HuggingFace:\n",
        "Transformers is a library of pretrained natural language processing, computer vision, audio, and multimodal models for inference and training.\n",
        " Use Transformers to train models on your data, build inference applications, and generate text with large language models. (https://huggingface.co/docs/transformers/en/index#design)\n",
        "'''\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "-3SuxTd0wGmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "summarize_article is a function that help us to summarize our articles\n",
        "It takes article as input and return its summary.\n",
        "The engine of this function is Transformer pipeline that used t5-small model for summarization. The reason of using t5-small is that,\n",
        "it is small and we can run it in this enviroment easily.\n",
        "summarize pipeline takes article and according to the lenghs, it return maximum 150 words and minmum 40 words a sentence.\n",
        "'''\n",
        "def summarize_article(article):\n",
        "  summarize = pipeline(\"summarization\", model=\"t5-small\") # Pipeline takes our task and type of model to create our summarization engine\n",
        "  summary = summarize(article, max_length=150, min_length=40, do_sample=False)[0]['summary_text'] # summarization engine takes input text, max and min lengths for summarize.\n",
        "  return summary"
      ],
      "metadata": {
        "id": "Hh2CCvR1whES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "sentiment_analysis is a function that help us to calculate emotion of text.\n",
        "It takes text as input and return emotion of it.\n",
        "In this function we used t5-small that is one of piplines in transformers library.\n",
        "'''\n",
        "def sentiment_analysis(text):\n",
        "  sentiment_analyzer= pipeline(\"sentiment-analysis\", model='t5-small') # Pipeline takes our task and type of model to create our sentiment analysis engine\n",
        "  sentiment = sentiment_analyzer(text)[0]\n",
        "  return sentiment"
      ],
      "metadata": {
        "id": "ydj8GiHt5jH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"\"\"\n",
        "Global surface temperature has increased faster since 1970 than in any other 50-year period over at least the last 2000 years.\n",
        "Based on the global average temperature for the most recent 10-year period (2014- 2023), the Earth is now about 1.2¬∞C warmer than it was in the pre-industrial era (1850- 1900). 2023 was the warmest year on record, with the global average near-surface temperature 1.45¬∞C above the pre-industrial baseline. The period 2011-2020 was the warmest decade on record for both land and ocean.\n",
        "Monthly and annual breaches of 1.5¬∞C do not mean that the world has failed to achieve the Paris Agreement‚Äôs temperature goal, which refers to a long-term temperature increase over decades, not individual months or years. Temperatures for any single month or year fluctuate due to natural variability, including El Ni√±o/La Ni√±a and volcanic eruptions. Consequently, long-term temperature changes are typically considered on decadal timescales.\n",
        "On an average day in 2023, nearly one third of the ocean was gripped by a marine heatwave. Over 90 per cent of the ocean experienced heatwave conditions at some point during 2023. Glaciers around the world thinned by an average of one meter per year and sea level rose at a rate of 4.5mm per year between 2011 and 2020. Greenland and Antarctica lost 38 per cent more ice during the period 2011-2020 than during 2001- 2010.\n",
        "Every fraction of a degree of warming matters. With every additional increment of global warming, changes in extremes and risks become larger. For example, every 0.1¬∞C increase in global warming causes clearly discernible increases in the intensity and frequency of temperature and precipitation extremes, as well as agricultural and ecological droughts in some regions.\n",
        "Greenhouse gas emissions reached a new record high of 57.4 gigatonnes in 2023. They must drop by 43 per cent by 2030 (compared to 2019 levels) to keep temperature increase from exceeding 1.5¬∞C. Under current national climate plans, the world is on track for a global average temperature rise of 2.5-2.9¬∞C above pre-industrial levels.\n",
        "Greenhouse gas concentrations in the atmosphere, already at their highest levels in 2 million years, have continued to rise. Global concentrations of carbon dioxide are now a full 50 per cent higher than they were in the pre-industrial era.\n",
        "The emissions gap in 2030, or the difference between necessary carbon dioxide reduction and current trends, is estimated at 21-24 gigatons of carbon dioxide equivalent (Gt CO2e) to limit global warming to 1.5¬∞C.\n",
        "To ensure a safe and liveable planet, experts say humanity must phase out global coal production and use by 2040, and reduce oil and gas production and use by three- quarters between 2020 and 2050.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarize_article(article)\n",
        "print(f\"Summary: {summary}\")\n",
        "\n",
        "sentiment = sentiment_analysis(summary)\n",
        "print(f\"Sentiment: {sentiment}\")\n"
      ],
      "metadata": {
        "id": "OqNAWjEE6qXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LangChain**\n",
        "\n",
        "Here we will try to implement a Mulit stage reasoning model (summarization and sentiment analysis) using langchain."
      ],
      "metadata": {
        "id": "VUu9S5ioAq45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To do this initialy we will install langchain, openai, langchain_community, huggingface, and Transformers\n",
        "'''\n",
        "!pip install langchain openai langchain_community\n",
        "!pip install huggingface-hub==0.16.4\n",
        "!pip install transformers==4.33.3"
      ],
      "metadata": {
        "id": "d-aYh-38A_uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''According to the langchain website, it has multitute of modules like llms and so fort and so on.\n",
        "it has also a community that some guys developed some function in this way\n",
        "The reason of using OpenAI and HuggingFaceEndpoint is that the server of some LLMs are not on google colab.\n",
        "'''\n",
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate # Prompt templates help to translate user input and parameters into instructions for a language model.\n",
        "# This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.llms import HuggingFaceEndpoint # Huggingface Endpoints: The Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demo apps (Spaces),\n",
        "# all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n",
        "from langchain_community.chat_models.huggingface import ChatHuggingFace # This will help us getting started with langchain_huggingface chat models"
      ],
      "metadata": {
        "id": "pfl0VF7_BqhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "To use HuggingFace or OpenAI facilities we have to get permission or access form its websites.\n",
        "To do this, we must loge in on Hugging face (https://huggingface.co/settings/) and openai websites and then make new tokens.\n",
        "By doing this we will able to use LLMs that google colab doesn't have them init.\n",
        "'''\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"xxx\"\n",
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \".....\""
      ],
      "metadata": {
        "id": "5zQXCGJ8EYI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Here we will able to summon models in each OpenAI and HuggingFace.\n",
        "OpenAI is not free.\n",
        "'''\n",
        "# llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "# llm = HuggingFaceEndpoint(repo_id=\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"summarization\",\n",
        "    huggingfacehub_api_token=\".....\"\n",
        ")"
      ],
      "metadata": {
        "id": "RLjMoprjIAHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "After picking our model from HuggingFace we need to define our prompt template.\n",
        "This defination allows us to make a multi stage enviroment for our task.\n",
        "'''\n",
        "# summarization task\n",
        "'''\n",
        "In this template we must tell the model what should it does. For instance, \"Summarize the following article:\" and then inject input.\n",
        "'''\n",
        "summarization_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=\"Summarize the following article in 50 words:\\n\\n{article}\\n\\nSummary:\"\n",
        ")\n",
        "\n",
        "# sentiment task\n",
        "'''\n",
        "In this template we must tell the model what should it does. For instance, \"Analyze the sentiment of the following text:\" and then inject input.\n",
        "'''\n",
        "sentiment_analysis_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"summary\"],\n",
        "    template=\"Analyze the sentiment of the following text:\\n\\n{summary}\\n\\nSentiment:\"\n",
        ")"
      ],
      "metadata": {
        "id": "pZlX2s4eI6Q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "After defining our templates, we will able to make chains that will perform our tasks.\n",
        "To do this, we use LLMChain function that takes type of Large Language model and our prompt template.\n",
        "'''\n",
        "\n",
        "summarization_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=summarization_prompt_template\n",
        ")\n",
        "\n",
        "sentiment_analysis_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=sentiment_analysis_prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "YaDVfCffKX8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Now by defining a function we will have a multi stage agent that will do our specific tasks.\n",
        "'''\n",
        "def process_article(article):\n",
        "    summary = summarization_chain.run(article)\n",
        "    sentiment = sentiment_analysis_chain.run(summary)\n",
        "    return summary, sentiment\n"
      ],
      "metadata": {
        "id": "YCV0TEzpKbQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"\"\"\n",
        "Global surface temperature has increased faster since 1970 than in any other 50-year period over at least the last 2000 years.\n",
        "Based on the global average temperature for the most recent 10-year period (2014- 2023), the Earth is now about 1.2¬∞C warmer than it was in the pre-industrial era (1850- 1900). 2023 was the warmest year on record, with the global average near-surface temperature 1.45¬∞C above the pre-industrial baseline. The period 2011-2020 was the warmest decade on record for both land and ocean.\n",
        "Monthly and annual breaches of 1.5¬∞C do not mean that the world has failed to achieve the Paris Agreement‚Äôs temperature goal, which refers to a long-term temperature increase over decades, not individual months or years. Temperatures for any single month or year fluctuate due to natural variability, including El Ni√±o/La Ni√±a and volcanic eruptions. Consequently, long-term temperature changes are typically considered on decadal timescales.\n",
        "On an average day in 2023, nearly one third of the ocean was gripped by a marine heatwave. Over 90 per cent of the ocean experienced heatwave conditions at some point during 2023. Glaciers around the world thinned by an average of one meter per year and sea level rose at a rate of 4.5mm per year between 2011 and 2020. Greenland and Antarctica lost 38 per cent more ice during the period 2011-2020 than during 2001- 2010.\n",
        "Every fraction of a degree of warming matters. With every additional increment of global warming, changes in extremes and risks become larger. For example, every 0.1¬∞C increase in global warming causes clearly discernible increases in the intensity and frequency of temperature and precipitation extremes, as well as agricultural and ecological droughts in some regions.\n",
        "Greenhouse gas emissions reached a new record high of 57.4 gigatonnes in 2023. They must drop by 43 per cent by 2030 (compared to 2019 levels) to keep temperature increase from exceeding 1.5¬∞C. Under current national climate plans, the world is on track for a global average temperature rise of 2.5-2.9¬∞C above pre-industrial levels.\n",
        "Greenhouse gas concentrations in the atmosphere, already at their highest levels in 2 million years, have continued to rise. Global concentrations of carbon dioxide are now a full 50 per cent higher than they were in the pre-industrial era.\n",
        "The emissions gap in 2030, or the difference between necessary carbon dioxide reduction and current trends, is estimated at 21-24 gigatons of carbon dioxide equivalent (Gt CO2e) to limit global warming to 1.5¬∞C.\n",
        "To ensure a safe and liveable planet, experts say humanity must phase out global coal production and use by 2040, and reduce oil and gas production and use by three- quarters between 2020 and 2050.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Mkm06IumKjbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the article\n",
        "summary, sentiment = process_article(article)\n",
        "print(f\"Summary: {summary}\")\n",
        "print(f\"Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "Lp32kqUrSERg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lang Chain 2nd part**\n",
        "\n",
        "In the previous section, we develop a multi stage agent that helped us to summarize and sentiment analysis on our text. Here we will try develop an agent to answer our question by searching on internet and its knowledge"
      ],
      "metadata": {
        "id": "6zTKJZQ3UiLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some libraries need to install for implementation. Some of them is about searching tools from internet like httpx, wikipedia, etc.\n",
        "!pip install -U openai httpcore httpx typing-extensions pydantic langchain\n",
        "!pip install -U langchain-experimental\n",
        "!pip install -U wikipedia google-search-results sqlalchemy\n",
        "\n",
        "# To have a efficent implementation without any error. We need to install partical version of some library.\n",
        "!pip install --force-reinstall pydantic==1.10.8\n",
        "!pip install --force-reinstall typing-inspect==0.8.0 typing_extensions==4.5.\n",
        "!pip install --force-reinstall chromadb==0.3.26\n",
        "\n",
        "!pip install huggingface-hub==0.16.4\n",
        "!pip install transformers==4.33.3"
      ],
      "metadata": {
        "id": "M3s1jyzFSJOx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import openai,os,json\n",
        "import pandas as pd\n",
        "from langchain import OpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, ConversationalRetrievalChain, ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import messages_from_dict, messages_to_dict\n",
        "from langchain.memory.chat_message_histories.in_memory import ChatMessageHistory\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "from io import StringIO\n",
        "import sys\n",
        "from typing import Dict, Optional\n",
        "\n",
        "from langchain.agents import load_tools\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain_community.chat_models.huggingface import ChatHuggingFace"
      ],
      "metadata": {
        "id": "ub8WkPAFUz70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply some setting on pandas library\n",
        "pd.set_option('display.max_column', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_seq_items', None)\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "pd.set_option('expand_frame_repr', True)"
      ],
      "metadata": {
        "id": "5UaQlKxiU2uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dedicating API Tokens form OpenAI, Serpapi (https://serper.dev/api-key), and HuggingFace.\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"xxx\"\n",
        "# os.environ[\"SERPAPI_API_KEY\"] = \".....\" # Serpapi assign a web searching mechanisim on our model. it has limitation so you should consider it.\n",
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \".......\""
      ],
      "metadata": {
        "id": "2zzkqKZuU4mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Repl allows us to run python code during searching. To have this atribute we have to write a class and use Tool from \"langchain.agent\" library\n",
        "\n",
        "class PythonREPL:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def run(self, command: str) -> str:\n",
        "        sys.stderr.write(\"EXECUTING PYTHON CODE:\\n---\\n\" + command + \"\\n---\\n\")\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = mystdout = StringIO()\n",
        "        try:\n",
        "            exec(command, globals())\n",
        "            sys.stdout = old_stdout\n",
        "            output = mystdout.getvalue()\n",
        "        except Exception as e:\n",
        "            sys.stdout = old_stdout\n",
        "            output = str(e)\n",
        "        sys.stderr.write(\"PYTHON OUTPUT: \\\"\" + output + \"\\\"\\n\")\n",
        "        return output\n",
        "\n",
        "\n",
        "python_repl = Tool(\n",
        "        \"Python REPL\",\n",
        "        PythonREPL().run,\n",
        "        \"\"\"A Python shell. Use this to execute python commands. Input should be a valid python command.\n",
        "        If you expect output it should be printed out.\"\"\",\n",
        "    )\n",
        "tools_py = [python_repl] # List of Tools"
      ],
      "metadata": {
        "id": "lzXvTz8JU7Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "llm = HuggingFaceEndpoint(repo_id=\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "\n",
        "tools = load_tools([\"wikipedia\", \"serpapi\", \"terminal\"], llm=llm, allow_dangerous_tools=True) # Some existing tool can summon with load tools. \"wikipedia\": searching in wikipedia, \"serpapi\": searching in entire website, \"terminal\":executing output in terminal\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools + tools_py, #merge all tools together\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "CqErq7I0U9Ek",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\n",
        "    \"Create a sample fake timeseries data for an airport using searching on the web. You need to plot your results.\" # our command for model\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "e2KTErF7U-sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\n",
        "    \"plot a sinsuse chart.\" # our command for model\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OppTejKzbaG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run LLM Localy on Google Colab using GPU**\n",
        "\n",
        "Here we will try to have an implementation for Single stage reasoning model (an assitant) on Google Colab using LangChain"
      ],
      "metadata": {
        "id": "Tq8MxbKIh0TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai langchain_community transformers langchain_huggingface"
      ],
      "metadata": {
        "id": "VCwxMUIJhRFH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "hf = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"gpt2\",\n",
        "    task=\"text-generation\", # type of task\n",
        "    pipeline_kwargs={\"max_new_tokens\": 30}, # number of words that has access to generate\n",
        "    device=0,\n",
        ")"
      ],
      "metadata": {
        "id": "ni8-a6uYigw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | hf\n",
        "\n",
        "question = \"What is electroencephalography?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question})) # Answer our question from the model"
      ],
      "metadata": {
        "id": "I1sZN0cYilAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | hf\n",
        "\n",
        "question = \"Is the weather cold in Canada?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question})) # Answer our question from the model"
      ],
      "metadata": {
        "id": "iU6LpdZ6imlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run LLM based on chromadb and langchain**\n",
        "\n",
        "We want to develop a model that can read our documnets like PDFs of an organization and answer our question by it."
      ],
      "metadata": {
        "id": "w3VY9_fwsWJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insatall useful libraries\n",
        "!pip install langchain langchain_community pypdf chromadb langchain_huggingface openai tiktoken huggingface_hub\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vw_wrU-Wsrm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To read our PDF documents we need import few libraries like PyPDFDirectoryLoader\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
        "import os\n",
        "import shutil\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# assign hugging face token\n",
        "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"..........\""
      ],
      "metadata": {
        "id": "Ogw68Wl9s6zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = r\"/content/drive/MyDrive/Large Language Model (LLM)/Developing My Knowledge In LLM/FaraDars/Data\"\n",
        "\n",
        "def load_documents():\n",
        "    '''\n",
        "    Load document is a function that allows us to read all pdfs in our directory.\n",
        "    It takes data path and return documents.\n",
        "    The type of output should be a list with the information of all pdfs\n",
        "    '''\n",
        "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
        "    return document_loader.load()\n",
        "\n",
        "documents = load_documents()\n",
        "print(documents)\n",
        "print(\"\\n\",\"The type of out is: \",type(documents))"
      ],
      "metadata": {
        "id": "eIMJU3Kxs8iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "As we need to inject our data into the model so that we can answer questions by them.\n",
        "To do this, one of methods is splitting all text into sub-text (separate paragraphs or chunks).\n",
        "So, we have a document and we must turn it into little chunks. We will able to do it by \"RecursiveCharacterTextSplitter\".\n",
        "In this method we can determine how much will be each chunk or paragraph.\n",
        "The reason of using this technique is that we should make a database (a vector based database).\n",
        "'''\n",
        "def split_text(documents: list[Document]):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=400,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "\n",
        "    document = chunks[10]\n",
        "    print(document.page_content)\n",
        "    print(document.metadata)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "chunks = split_text(documents)\n",
        "print('\\n',100*'-')\n",
        "for chunk in chunks[:10]:\n",
        "    print(chunk)"
      ],
      "metadata": {
        "id": "slL7iKUstA-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we will apply vectorization method (Chromadb) on our dataset to make a database.\n",
        "CHROMA_PATH = \"/content/drive/MyDrive/Large Language Model (LLM)/Developing My Knowledge In LLM/FaraDars/Data/chromadb\"\n",
        "def save_to_chroma(chunks: list[Document]):\n",
        "    if os.path.exists(CHROMA_PATH):\n",
        "        shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "    db = Chroma.from_documents(\n",
        "        chunks, HuggingFaceEmbeddings(), persist_directory=CHROMA_PATH\n",
        "    ) # here we used HuggingFaceEmbeddings insted of sentence transformer method.\n",
        "    db.persist()\n",
        "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
      ],
      "metadata": {
        "id": "lv2HAioKtHcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data_store():\n",
        "    '''\n",
        "    generate_data_store is a function that integrat our 3 function (read, split, and save as vector)\n",
        "    '''\n",
        "    documents = load_documents()\n",
        "    chunks = split_text(documents)\n",
        "    save_to_chroma(chunks)\n",
        "\n",
        "generate_data_store()"
      ],
      "metadata": {
        "id": "aOahcokk7x0n",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It is just a test to see how HuggingFaceEmbeddings work\n",
        "ex = \"apple\"\n",
        "ex_1 = \"orange\"\n",
        "ex_2 = \"iphone\"\n",
        "\n",
        "embedding_function = HuggingFaceEmbeddings()\n",
        "vector = embedding_function.embed_query(ex)\n",
        "vector_1 = embedding_function.embed_query(ex_1)\n",
        "vector_2 = embedding_function.embed_query(ex_2)\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "IUjqNeHjtHUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on our documents we will able to answer questions. For instance, regarding that our documents are about MRI motions, we need write a question in this way.\n",
        "Then we have to make Prompt template to take our question and sementically find good chunks that are meaningfully near to our question.**"
      ],
      "metadata": {
        "id": "QOjRJhPsAQ3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_text = \"How many motion we have in MR imaging\"\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question based on the above context: {question}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "tlleqW8ttNKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "For finding similarity between our question and chunks, we initially must embed our question.\n",
        "Then we will able to it by \" similarity_search_with_relevance_scores \".\n",
        "'''\n",
        "embedding_function = HuggingFaceEmbeddings()\n",
        "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
        "\n",
        "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
        "\n",
        "if len(results) == 0 or results[0][1] < 0.1:\n",
        "    print(f\"Unable to find matching results.\")"
      ],
      "metadata": {
        "id": "ON3zPm5G_BYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results]) # stick all 3 text - \"\\n\\n---\\n\\n help to sperate each sentence\"\n",
        "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE) # turn our prompt template to chat propmpt template\n",
        "prompt = prompt_template.format(context=context_text, question=query_text)\n",
        "print(prompt)\n",
        "\n",
        "'''\n",
        "All this prompt will inject to a LLM. Based on our question, extracted information from our database, and knowledge of LLM model, model will answer the questio\n",
        "'''"
      ],
      "metadata": {
        "id": "fbTuyom4_BSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"https://huggingface.co/mradermacher/dolphin-2.9.3-mistral-nemo-12b-llamacppfixed-GGUF\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NDI8NU8XlClk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "mIFlBj13YM38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    # model_id=\"openai-community/gpt2\",\n",
        "    model_id=\"MaziyarPanahi/Llama-3-Groq-8B-Tool-Use-GGUF\",\n",
        "    # model_id=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.03,\n",
        "        token = \".....\",\n",
        "    ),\n",
        ")\n",
        "model = ChatHuggingFace(llm = llm)\n",
        "response_text = model.predict(prompt)\n",
        "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
        "formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
        "print(formatted_response)"
      ],
      "metadata": {
        "id": "bSiWYTEetNCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_text"
      ],
      "metadata": {
        "id": "pIKiGTou_RAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-C0PTlAI5-6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}